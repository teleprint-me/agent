# PKGBUILD for llama‑cpp built from source using the Vulkan backend.
#
# This is a personal build script; it does not aim to be an official package,
# but it should compile reproducibly on any Arch system that has the listed
# dependencies installed.

#
# Metadata
#

# name of the generated package (no hyphens)
pkgname=llama-cpp 
# overridden by pkgver() – short Git SHA
pkgver=cdbada8d10
# release number for this repo snapshot
pkgrel=1
# supported architectures
arch=('x86_64')
# source url
url='https://github.com/ggml-org/llama.cpp'
# source license
license=('MIT')

#
# Description
#

pkgdesc="Port of Facebook's LLaMA model in C/C++ (using Vulkan backend)"

#
# Dependencies
#

# Packages required at build time
makedepends=(
  cmake          # configure & compile the project
  git            # to fetch source from VCS
  shaderc        # optional: for compiling GLSL shaders
  vulkan-headers # compiler headers; needed by CMake's FindVulkan.cmake
)

# Runtime dependencies (the package will pull these in automatically)
depends=(
  glibc             # the base system libc
  gcc-libs          # runtime libgcc, etc.
  python            # Python interpreter for optional utilities
  vulkan-icd-loader # Vulkan loader on Linux
  curl              # used by llama.cpp tools to fetch model files
)

# Optional extras – users can choose whether they want them or not
optdepends=(
  'vulkan-tools: Vulkan tools and utilities'
  'vulkan-validation-layers: Validation layers for debugging GPU code'
  'python-numpy: Scientific Python package (used by quantize)'
  'python-pytorch: Torch tensors & dynamic neural nets'
  'python-safetensors: Simple, safe way to store/distribute tensors (aur)'
  'python-sentencepiece: Tokenizer for neural‑network text generation (aur)'
  'python-transformers: State‑of‑the art ML model framework (aur)'
  'python-gguf: GGUF file format writer in Python (aur)'
)

# Provide the same name as the package itself
provides=("$pkgname")

# Avoid multiple builds of different variants clashing on /usr
# NOTEs: 
#   - There are other packages (they keep multiplying)
#   - Listing every possible variant here seems unreasonable.
conflicts=(
    "$pkgname"
    ggml ggml-git
    libggml libggml-git
    llama.cpp llama.cpp-git
    llama.cpp-cuda llama.cpp-cuda-git
    llama.cpp-hip llama.cpp-hip-git
    llama.cpp-vulkan llama.cpp-vulkan-git
)

#
# Source
#

# VCS source – the short SHA is taken from pkgver()
source=("git+https://github.com/ggml-org/llama.cpp.git")

# SKIP because this is a rolling git build
sha256sums=('SKIP')

# No debug info, strip binaries by default
options=('!debug' 'strip')

# Override the version with the short git SHA
pkgver() {
  cd "$srcdir/llama.cpp"
  echo "$(git rev-parse --short HEAD)"
}

# Build the project
build() {
  cd "$srcdir/llama.cpp"
  cmake -B build \
    -DCMAKE_BUILD_TYPE=Release \ # Debug mode degrades performance
    -DGGML_VULKAN=1 \ # Build using Vulkan backend
    -DGGML_VULKAN_DEBUG=0 \ # Disable Vulkan debug mode
    -DGGML_DEBUG=OFF \ # disable symbols (enabling this degrades performance)
    -DLLAMA_BUILD_TESTS=OFF \ # disable tests (only useful for dev builds)
    -DLLAMA_BUILD_EXAMPLES=OFF \ # disable example bins (extra, not required)
    -DLLAMA_BUILD_COMMON=ON \ # tools depends on common (enables curl)
    -DLLAMA_BUILD_TOOLS=ON \ # enable tools (llama-server, llama-quantize, etc)
    -DBUILD_SHARED_LIBS=ON # required prereq (static builds are optional)
  cmake --build build -j "$(nproc)"
}

# Install files into the makepkg staging dir
package() {
  DESTDIR="$pkgdir" cmake --install "$srcdir/llama.cpp/build"
}

